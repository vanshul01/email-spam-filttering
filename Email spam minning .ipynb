{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Email spam minning .ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPhf8HvuDhaoWBnRbBEmGm+"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"NR-2mPqWcDxY"},"source":["**Tokenization**\n","\n","Tokenization is the first step in NLP. It is the process of breaking strings into tokens which in turn are small structures or units. Tokenization involves three steps which are breaking a complex sentence into words, understanding the importance of each word with respect to the sentence and finally produce structural description on an input sentence."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eGnBPpukdGRc","executionInfo":{"status":"ok","timestamp":1606543988711,"user_tz":-330,"elapsed":3871,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"5c8b3864-16d5-4296-f96a-6b33c399c4b6"},"source":["pip install nltk"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.15.0)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6WSbbuIDbvT7","executionInfo":{"status":"ok","timestamp":1606544025758,"user_tz":-330,"elapsed":1710,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"a52c5e72-1b08-4a7e-e2c5-959c91a295d4"},"source":["# Importing necessary library\n","\n","import pandas as pd\n","import numpy as np\n","import nltk\n","import os\n","import nltk.corpus\n","nltk.download('punkt')\n","# sample text for performing tokenization\n","text = \"In Brazil they drive on the right-hand side of the road. Brazil has a large coastline on the eastern side of South America\"\n","# importing word_tokenize from nltk\n","from nltk.tokenize import word_tokenize\n","# Passing the string text into word tokenize for breaking the sentences\n","token = word_tokenize(text)\n","token"],"execution_count":9,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["['In',\n"," 'Brazil',\n"," 'they',\n"," 'drive',\n"," 'on',\n"," 'the',\n"," 'right-hand',\n"," 'side',\n"," 'of',\n"," 'the',\n"," 'road',\n"," '.',\n"," 'Brazil',\n"," 'has',\n"," 'a',\n"," 'large',\n"," 'coastline',\n"," 'on',\n"," 'the',\n"," 'eastern',\n"," 'side',\n"," 'of',\n"," 'South',\n"," 'America']"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"aRfoRI4yf3Mf"},"source":["**Finding frequency distinct in the text**"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDkSnx1afvzp","executionInfo":{"status":"ok","timestamp":1606544258505,"user_tz":-330,"elapsed":1284,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"cbcf5fd7-439f-4bcb-e56c-7cbfa410375f"},"source":["# finding the frequency distinct in the tokens\n","# Importing FreqDist library from nltk and passing token into FreqDist\n","from nltk.probability import FreqDist\n","fdist = FreqDist(token)\n","fdist"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["FreqDist({'.': 1,\n","          'America': 1,\n","          'Brazil': 2,\n","          'In': 1,\n","          'South': 1,\n","          'a': 1,\n","          'coastline': 1,\n","          'drive': 1,\n","          'eastern': 1,\n","          'has': 1,\n","          'large': 1,\n","          'of': 2,\n","          'on': 2,\n","          'right-hand': 1,\n","          'road': 1,\n","          'side': 2,\n","          'the': 3,\n","          'they': 1})"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NQe-PKwFgFgm","executionInfo":{"status":"ok","timestamp":1606544287965,"user_tz":-330,"elapsed":1334,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"55b85f68-b5b6-469e-9181-7026cf52a0c8"},"source":["# To find the frequency of top 10 words\n","fdist1 = fdist.most_common(10)\n","fdist1"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('the', 3),\n"," ('Brazil', 2),\n"," ('on', 2),\n"," ('side', 2),\n"," ('of', 2),\n"," ('In', 1),\n"," ('they', 1),\n"," ('drive', 1),\n"," ('right-hand', 1),\n"," ('road', 1)]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"markdown","metadata":{"id":"nF3NXwTbgJxx"},"source":["**Stemming** \n","\n","There are two methods in Stemming namely, Porter Stemming (removes common morphological and inflectional endings from words) and Lancaster Stemming (a more aggressive stemming algorithm)."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"nTO7tjUygMIx","executionInfo":{"status":"ok","timestamp":1606544436742,"user_tz":-330,"elapsed":1241,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"0798b15d-9689-47dd-9ab4-f412fcfde298"},"source":["# Importing Porterstemmer from nltk library\n","# Checking for the word ‘giving’ \n","from nltk.stem import PorterStemmer\n","pst = PorterStemmer()\n","pst.stem(\"waiting\")"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'wait'"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UpcEjL_rgr0V","executionInfo":{"status":"ok","timestamp":1606544456051,"user_tz":-330,"elapsed":1375,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"21121525-e44d-4b18-a9b0-848e09e5d782"},"source":["# Checking for the list of words\n","stm = [\"waited\", \"waiting\", \"waits\"]\n","for word in stm :\n","   print(word+ \":\" +pst.stem(word))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["waited:wait\n","waiting:wait\n","waits:wait\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mq-VsDm8gwdC","executionInfo":{"status":"ok","timestamp":1606544529891,"user_tz":-330,"elapsed":1429,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"8eeb63c3-0f91-4fdd-ba1b-e798b06cffbe"},"source":["# Importing LancasterStemmer from nltk\n","from nltk.stem import LancasterStemmer\n","lst = LancasterStemmer()\n","stm = [\"giving\", \"given\", \"given\", \"gave\"]\n","for word in stm :\n"," print(word+ \":\" +lst.stem(word))"],"execution_count":17,"outputs":[{"output_type":"stream","text":["giving:giv\n","given:giv\n","given:giv\n","gave:gav\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Ja7BgkaphMAU"},"source":["**Lemmatization**\n","\n","In simpler terms, it is the process of converting a word to its base form. The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l0dh5XHPhCn6","executionInfo":{"status":"ok","timestamp":1606544746942,"user_tz":-330,"elapsed":3727,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"854e83ac-8f88-4bc7-ee0a-a9d428ca77d9"},"source":["# Importing Lemmatizer library from nltk\n","import nltk\n","nltk.download('wordnet')\n","from nltk.stem import WordNetLemmatizer\n","lemmatizer = WordNetLemmatizer() \n"," \n","print(\"rocks :\", lemmatizer.lemmatize(\"rocks\")) \n","print(\"corpora :\", lemmatizer.lemmatize(\"corpora\"))"],"execution_count":19,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/wordnet.zip.\n","rocks : rock\n","corpora : corpus\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"4UMVC6pfh7dh"},"source":["**Stop Words**\n","\n","“Stop words” are the most common words in a language like “the”, “a”, “at”, “for”, “above”, “on”, “is”, “all”. These words do not provide any meaning and are usually removed from texts. We can remove these stop words using nltk library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kbaPN0P4h3y2","executionInfo":{"status":"ok","timestamp":1606544880895,"user_tz":-330,"elapsed":1292,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"e022d9c6-404b-435d-cc55-dfedbabde5bf"},"source":["# importing stopwors from nltk library\n","import nltk\n","nltk.download('stopwords')\n","from nltk import word_tokenize\n","from nltk.corpus import stopwords\n","a = set(stopwords.words('english'))\n","text = \"Cristiano Ronaldo was born on February 5, 1985, in Funchal, Madeira, Portugal.\"\n","text1 = word_tokenize(text.lower())\n","print(text1)\n","stopwords = [x for x in text1 if x not in a]\n","print(stopwords)"],"execution_count":23,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","['cristiano', 'ronaldo', 'was', 'born', 'on', 'february', '5', ',', '1985', ',', 'in', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n","['cristiano', 'ronaldo', 'born', 'february', '5', ',', '1985', ',', 'funchal', ',', 'madeira', ',', 'portugal', '.']\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hRK2x9VrjJIG"},"source":["**Part of speech tagging (POS)**\n","\n","Part-of-speech tagging is used to assign parts of speech to each word of a given text (such as nouns, verbs, pronouns, adverbs, conjunction, adjectives, interjection) based on its definition and its context. There are many tools available for POS taggers and some of the widely used taggers are NLTK, Spacy, TextBlob, Standford CoreNLP, etc.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7kI0f8AljatE","executionInfo":{"status":"ok","timestamp":1606545295884,"user_tz":-330,"elapsed":1212,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"04664ba0-6a7b-491e-e13a-c44d81b52c0a"},"source":["import nltk\n","nltk.download('averaged_perceptron_tagger')\n","text = \"vote to choose a particular man or a group (party) to represent them in parliament\"\n","#Tokenize the text\n","tex = word_tokenize(text)\n","for token in tex:\n","  print(nltk.pos_tag([token]))"],"execution_count":27,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[('vote', 'NN')]\n","[('to', 'TO')]\n","[('choose', 'NN')]\n","[('a', 'DT')]\n","[('particular', 'JJ')]\n","[('man', 'NN')]\n","[('or', 'CC')]\n","[('a', 'DT')]\n","[('group', 'NN')]\n","[('(', '(')]\n","[('party', 'NN')]\n","[(')', ')')]\n","[('to', 'TO')]\n","[('represent', 'NN')]\n","[('them', 'PRP')]\n","[('in', 'IN')]\n","[('parliament', 'NN')]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6R4axLeNkVXe"},"source":["**Named entity recognition**\n","\n","It is the process of detecting the named entities such as the person name, the location name, the company name, the quantities and the monetary value."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":491},"id":"ktZVZZi4kjcU","executionInfo":{"status":"ok","timestamp":1606545552302,"user_tz":-330,"elapsed":1514,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"4330c85d-51d4-48e2-dafe-4aa86d24d8cb"},"source":["text = \"Google’s CEO Sundar Pichai introduced the new Pixel at Minnesota Roi Centre Event\"\n","#importing chunk library from nltk\n","import nltk\n","nltk.download('maxent_ne_chunker')\n","nltk.download('words')\n","from nltk import ne_chunk\n","# tokenize and POS Tagging before doing chunk\n","token = word_tokenize(text)\n","tags = nltk.pos_tag(token)\n","chunk = ne_chunk(tags)\n","chunk"],"execution_count":31,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package maxent_ne_chunker to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/words.zip.\n"],"name":"stdout"},{"output_type":"error","ename":"TclError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTclError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/formatters.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_real_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/tree.py\u001b[0m in \u001b[0;36m_repr_png_\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutil\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternals\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_binary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m         \u001b[0m_canvas_frame\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCanvasFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m         \u001b[0mwidget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree_to_treesegment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0m_canvas_frame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_widget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/nltk/draw/util.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, parent, **kw)\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;31m# If no parent was given, set up a top-level window.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1652\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1653\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NLTK'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1655\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'<Control-p>'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.6/tkinter/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, screenName, baseName, className, useTk, sync, use)\u001b[0m\n\u001b[1;32m   2021\u001b[0m                 \u001b[0mbaseName\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseName\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2022\u001b[0m         \u001b[0minteractive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2023\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tkinter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscreenName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclassName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwantobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msync\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2024\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0museTk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2025\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_loadtk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTclError\u001b[0m: no display name and no $DISPLAY environment variable"]},{"output_type":"execute_result","data":{"text/plain":["Tree('S', [Tree('PERSON', [('Google', 'NNP')]), ('’', 'NNP'), ('s', 'VBD'), Tree('ORGANIZATION', [('CEO', 'NNP'), ('Sundar', 'NNP'), ('Pichai', 'NNP')]), ('introduced', 'VBD'), ('the', 'DT'), ('new', 'JJ'), ('Pixel', 'NNP'), ('at', 'IN'), Tree('ORGANIZATION', [('Minnesota', 'NNP'), ('Roi', 'NNP'), ('Centre', 'NNP')]), ('Event', 'NNP')])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"markdown","metadata":{"id":"Qs6pl1_SleD4"},"source":["**Chunking**\n","\n","Chunking means picking up individual pieces of information and grouping them into bigger pieces. In the context of NLP and text mining, chunking means a grouping of words or tokens into chunks.\n"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"34BZ4Xv6lteI","executionInfo":{"status":"ok","timestamp":1606545803696,"user_tz":-330,"elapsed":1193,"user":{"displayName":"Vanshul Gupta","photoUrl":"","userId":"15289125268056915051"}},"outputId":"7133d363-0f25-47df-fb4f-fa74719d4eb8"},"source":["text = \"We saw the yellow dog\"\n","token = word_tokenize(text)\n","tags = nltk.pos_tag(token)\n","reg = \"NP: {<DT>?<JJ>*<NN>}\"\n","a = nltk.RegexpParser(reg)\n","result = a.parse(tags)\n","print(result)"],"execution_count":32,"outputs":[{"output_type":"stream","text":["(S We/PRP saw/VBD (NP the/DT yellow/JJ dog/NN))\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"P2N89pRnl_A_"},"source":["**Reference:**\n","\n","https://towardsdatascience.com/email-spam-detection-1-2-b0e06a5c0472\n","\n","https://www.kdnuggets.com/2017/03/email-spam-filtering-an-implementation-with-python-and-scikit-learn.html\n","\n"," \n","\n","https://www.youtube.com/watch?v=cNLPt02RwF0\n","\n","https://www.youtube.com/watch?v=exHwwy9kVcg"]}]}